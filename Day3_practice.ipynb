{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0DrQhgJlCU6"
      },
      "source": [
        "# Problem 5: DQN (Deep Q Network)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dcsQoO_ElCU7"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio\n",
        "\n",
        "from typing import Dict, List, Union, Tuple, Optional\n",
        "from copy import deepcopy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEnanL86lCU8"
      },
      "source": [
        "## Cart Pole\n",
        "\n",
        "### Observation Space\n",
        "The observation is a ndarray with shape (4,) with the values corresponding to the following positions and velocities:\n",
        "\n",
        "### Action Space\n",
        "The action is a ndarray with shape (1,) which can take values {0, 1} indicating the direction of the fixed force the cart is pushed with.\n",
        "\n",
        "$\\cdot$ 0: Push cart to the left\n",
        "\n",
        "$\\cdot$ 1: Push cart to the right\n",
        "\n",
        "Note: The velocity that is reduced or increased by the applied force is not fixed and it depends on the angle the pole is pointing. The center of gravity of the pole varies the amount of energy needed to move the cart underneath it\n",
        "\n",
        "### Rewards\n",
        "Since the goal is to keep the pole upright for as long as possible, a reward of +1 for every step taken, including the termination step, is allotted. The threshold for rewards is 500 for v1 and 200 for v0.\n",
        "\n",
        "Reference: https://gymnasium.farama.org/environments/classic_control/cart_pole/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "Cl_kOjTRlCU8",
        "outputId": "121e3415-98b9-4a04-da5f-5afb5de6eb20"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "obs, info = env.reset(seed=1)\n",
        "\n",
        "img = env.render()\n",
        "\n",
        "plt.imshow(img)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwnNOODklCU9"
      },
      "source": [
        "## Mountain Car\n",
        "\n",
        "### Observation Space\n",
        "The observation is a ndarray with shape (2,) with the values corresponding to the following positions and velocities:\n",
        "\n",
        "### Action Space\n",
        "There are 3 discrete deterministic actions:\n",
        "\n",
        "$\\cdot$ 0: Accelerate to the left\n",
        "\n",
        "$\\cdot$ 1: Don’t accelerate\n",
        "\n",
        "$\\cdot$ 2: Accelerate to the right\n",
        "\n",
        "Note: The velocity that is reduced or increased by the applied force is not fixed and it depends on the angle the pole is pointing. The center of gravity of the pole varies the amount of energy needed to move the cart underneath it\n",
        "\n",
        "### Rewards\n",
        "The goal is to reach the flag placed on top of the right hill as quickly as possible, as such the agent is penalised with a reward of -1 for each timestep.\n",
        "\n",
        "Reference: https://gymnasium.farama.org/environments/classic_control/mountain_car/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "jQ7L4AQxlCU9",
        "outputId": "28b80988-d6e6-4c3c-ba90-1089d8cedbaf"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "env = gym.make(\"MountainCar-v0\", render_mode=\"rgb_array\")\n",
        "obs, info = env.reset(seed=1)\n",
        "\n",
        "img = env.render()\n",
        "\n",
        "plt.imshow(img)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYzrtEZYlCU9"
      },
      "source": [
        "## Start Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUZE9xMdlCU9"
      },
      "source": [
        "In this lab, we will apply deep learning as function approximations in reinforcement learning.\n",
        "\n",
        "Reference: DQN https://arxiv.org/abs/1312.5602\n",
        "\n",
        "In tabular Q-learning, we maintain a table of state-action pairs $(s,a)$ and save one action value for each entry $Q(s,a),\\forall (s,a)$. At each time step $t$, we are in state $s_t$, then we choose action based on $\\epsilon$-greedy strategy. With prob $\\epsilon$, choose action uniformly random; with prob $1-\\epsilon$, choose action based on $$a_t = \\arg\\max_a Q(s_t,a)$$\n",
        "\n",
        "We then get the instant reward $r_t$, update the Q-table using the following rule\n",
        "\n",
        "$$Q(s_t,a_t) \\leftarrow (1-\\alpha)Q(s_t,a_t) + \\alpha (r_t + \\max_a \\gamma Q(s_{t+1},a))$$\n",
        "\n",
        "where $\\alpha \\in (0,1)$ is learning rate. The algorithm is shown to converge in tabular cases. However, in cases where we cannot keep a table for state and action, we need function approximation. Consider using neural network with parameter $\\theta$, the network takes as input state $s$ and action $a$. (*there are alternative parameterizations here*). Let $Q_\\theta(s,a)$ be the output of the network, to estimate the optimal action value function in state $s$ and take action $a$ (and follow optimal policy thereafter).\n",
        "\n",
        "$$Q_\\theta(s,a) \\approx Q^\\ast(s,a)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9p-UxdUlCU9"
      },
      "source": [
        "### Bellman optimality equation\n",
        "\n",
        "We will use Bellman optimality equation to find $\\theta$ such that the above approximation holds better. Recall that for optimal Q function $Q^\\ast(s,a)$ the following holds for all $(s,a)$\n",
        "\n",
        "$$Q^\\ast(s_t,a_t) = \\max_a \\mathbb{E}\\big[r_t + \\gamma Q^\\ast(s_{t+1},a)\\big]$$\n",
        "\n",
        "Hence a natural objective to consider is\n",
        "\n",
        "$$\\min_\\theta\\  \\left(Q_\\theta(s_t,a_t) - \\max_a \\mathbb{E}\\big[r_t + \\gamma Q_\\theta(s_{t+1},a)\\big]\\right)^2$$\n",
        "\n",
        "Let us first build a neural network $Q_\\theta(s,a)$ as required above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_9jWd_nPlCU9"
      },
      "outputs": [],
      "source": [
        "# Define the Q-network\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        output_dim: int,\n",
        "        hidden_dims:[List[int]],\n",
        "        activation: nn.Module = nn.ReLU,\n",
        "    ) -> None:\n",
        "        super(QNetwork, self).__init__()\n",
        "        hidden_dims = [input_dim] + list(hidden_dims)\n",
        "        model = []\n",
        "\n",
        "        #################################################################\n",
        "        #                             TODO                              #\n",
        "        #################################################################\n",
        "        \"\"\"\n",
        "        Model Sequential:\n",
        "        input -> hidden(1) -> hidden(2) -> ... -> hidden(last) -> output\n",
        "        with activation function(ReLU function)\n",
        "\n",
        "        Define the initial, hidden, and output layers\n",
        "        \"\"\"\n",
        "        for in_dim, out_dim in zip(hidden_dims[:-1], hidden_dims[1:]):\n",
        "            model += [nn.Linear(in_dim, out_dim), activation()]\n",
        "\n",
        "        model += [nn.Linear(hidden_dims[-1], output_dim)]\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "        #################################################################\n",
        "\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCYo8RHilCU-"
      },
      "source": [
        "We can hence collect a bunch of samples $(s_t,a_t,r_t,s_{t+1})$, and compute the targets using the current network. Let the target be $d_i$ as the $i$-th target\n",
        "\n",
        "$$d_i = \\max_a r_t + \\gamma Q_\\theta(s_{t+1},a)$$\n",
        "\n",
        "And then feed this value into the computational graph and minimize the Bellman error loss. This procedure has been shown to be fairly unstable. The reference paper has offered two techniques to stabilize the training process: target network and replay buffer.\n",
        "\n",
        "#### **1. Replay Buffer**\n",
        "Maintain a buffer $R$ to store trainsition tuples $(s_t,a_t,r_t,s_{t+1})$, when we minimize the Bellman error. When optimizing the Bellman error loss, we sample batches from the replay buffer and compute gradients for update on these batches. In particular, in each update, we sample $N$ tuples from buffer $(s_t,a_t,r_t,s_{t+1}) \\sim R$ and then compute\n",
        "loss\n",
        "\n",
        "$$\\frac{1}{N} \\sum_{i=1}^N (Q_\\theta(s_i,a_i) - \\max_a (r_i + \\gamma Q_\\theta(s_i^\\prime,a))^2$$\n",
        "\n",
        "and update parameters using backprop.\n",
        "\n",
        "#### **2. Target Network**\n",
        "Maintain a target network in addition to the original pricipal network. The target network is just a copy of the original network but the parameters are not updated by gradients. The target network $\\theta_{\\text{target}}$ is updated for every $\\tau$ time steps.\n",
        "\n",
        "$$\\theta_{\\text{target}} \\leftarrow (1-\\tau)*\\theta_{\\text{target}} + \\tau * \\theta_{\\text{update}}$$\n",
        "\n",
        "Target network is used to compute the targets for update\n",
        "\n",
        "$$d_i = \\max_a r_t + \\gamma Q_{\\theta^{-}}(s_{i}^\\prime,a)$$\n",
        "\n",
        "the targets are used in the loss function to update the principal network parameters. This slowly updated target network ensures that the targets come from a relatively stationary distribution and hence stabilize learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t82T2sSxlCU-"
      },
      "outputs": [],
      "source": [
        "# Define the DQN agent\n",
        "class DQNAgent:\n",
        "    def __init__(\n",
        "        self,\n",
        "        q_network: nn.Module,\n",
        "        optimizer: optim.Optimizer,\n",
        "        input_dim: int,\n",
        "        output_dim: int,\n",
        "        epsilon_start: float,\n",
        "        epsilon_final: float,\n",
        "        epsilon_decay: float,\n",
        "        target_update: int, # target network 업데이트 주기\n",
        "        batch_size: int,\n",
        "        memory_capacity: int, # replay buffer 크기\n",
        "        gamma: float, # discount factor\n",
        "        tau: float # target network 업데이트 크기\n",
        "    ) -> None:\n",
        "        self.q_network, self.target_network = q_network, deepcopy(q_network)\n",
        "        self.target_network.eval()\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.epsilon = epsilon_start\n",
        "        self.epsilon_final = epsilon_final\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "\n",
        "        self.target_update = target_update\n",
        "        self.batch_size = batch_size\n",
        "        self.memory_capacity = memory_capacity\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "\n",
        "        self.memory = []\n",
        "        self.steps = 0\n",
        "\n",
        "    def select_action(self, state):\n",
        "        #################################################################\n",
        "        #                             TODO                              #\n",
        "        #################################################################\n",
        "        \"\"\"\n",
        "        With probability epsilon, select a random action a_t\n",
        "        Otherwise, select a_t = argmax_a Q(s, a | theta)\n",
        "        \"\"\"\n",
        "        if np.random.rand() < self.epsilon:\n",
        "          return np.random.choice(self.output_dim, 1)\n",
        "        else:\n",
        "          return torch.argmax(self.q_network(state))\n",
        "\n",
        "\n",
        "        #################################################################\n",
        "\n",
        "    def store_experience(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "        if len(self.memory) > self.memory_capacity:\n",
        "            self.memory.pop(0)\n",
        "\n",
        "    def train(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        # Sample a mini-batch from the replay memory\n",
        "        batch = random.sample(self.memory, self.batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "        states = torch.FloatTensor(states)\n",
        "        actions = torch.LongTensor(actions)\n",
        "        rewards = torch.FloatTensor(rewards)\n",
        "        next_states = torch.FloatTensor(next_states)\n",
        "        dones = torch.FloatTensor(dones)\n",
        "\n",
        "        #################################################################\n",
        "        #                             TODO                              #\n",
        "        #################################################################\n",
        "        \"\"\"\n",
        "        Compute Q-values for the current states\n",
        "        \"\"\"\n",
        "        q_values = self.q_network[states]\n",
        "        q_values = q_values[actions]\n",
        "\n",
        "        #################################################################\n",
        "\n",
        "        #################################################################\n",
        "        #                             TODO                              #\n",
        "        #################################################################\n",
        "        \"\"\"\n",
        "        Compute Q-values for the next states using the target network\n",
        "        \"\"\"\n",
        "        next_q_values = torch.max(self.target_network[next_states], dim=1)\n",
        "\n",
        "        #################################################################\n",
        "\n",
        "        #################################################################\n",
        "        #                             TODO                              #\n",
        "        #################################################################\n",
        "        \"\"\"\n",
        "        Compute the expected Q-values\n",
        "        \"\"\"\n",
        "        expected_q_values = rewards + self.gamma*next_q_values\n",
        "\n",
        "        #################################################################\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = nn.MSELoss()(q_values, expected_q_values)\n",
        "\n",
        "        # Update the Q-network\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Update epsilon for exploration\n",
        "        #################################################################\n",
        "        #                             TODO                              #\n",
        "        #################################################################\n",
        "        \"\"\"\n",
        "        Compute the decay epsilon\n",
        "        \"\"\"\n",
        "        self.epsilon = self.epsilon*self.epsilon_decay\n",
        "\n",
        "        self.steps += 1\n",
        "        #################################################################\n",
        "        #                             TODO                              #\n",
        "        #################################################################\n",
        "        \"\"\"\n",
        "        Update the target network every 'target_update' steps\n",
        "        \"\"\"\n",
        "        if self.steps % self.target_update == 0:\n",
        "            for t, q in zip(self.target_network.parameters(), self.q_network.parameters()):\n",
        "                t.data.copy_((1 - self.tau)*t + self.tau*q)\n",
        "\n",
        "        #################################################################\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GNed6ORlCU-"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "epsilon_start = 1.0  # Initial epsilon for epsilon-greedy exploration\n",
        "epsilon_final = 0.01  # Final epsilon\n",
        "epsilon_decay = 0.995  # Epsilon decay rate\n",
        "target_update = 10  # Update the target network every 'target_update' steps\n",
        "memory_capacity = 10000  # Replay memory capacity\n",
        "num_episodes = 200  # Number of episodes to train\n",
        "\n",
        "#################################################################\n",
        "#                             TODO                              #\n",
        "#################################################################\n",
        "lr = 1e-3  # Learning rate\n",
        "gamma = 0.99  # Discount factor\n",
        "tau = 0.1 # Parameters update factor\n",
        "batch_size = 64  # Mini-batch size\n",
        "hidden_dims = [20, 20] # Hidden layers\n",
        "#################################################################\n",
        "\n",
        "\n",
        "# Initialize the environment and the agent\n",
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "input_dim = env.observation_space.shape[0]\n",
        "output_dim = env.action_space.n\n",
        "\n",
        "q_network = QNetwork(input_dim, output_dim, hidden_dims)\n",
        "optimizer = optim.Adam(q_network.parameters(), lr=lr)\n",
        "agent = DQNAgent(\n",
        "    q_network=q_network,\n",
        "    optimizer=optimizer,\n",
        "    input_dim=input_dim,\n",
        "    output_dim=output_dim,\n",
        "    epsilon_start=epsilon_start,\n",
        "    epsilon_final=epsilon_final,\n",
        "    epsilon_decay=epsilon_decay,\n",
        "    target_update=target_update,\n",
        "    batch_size=batch_size,\n",
        "    memory_capacity=memory_capacity,\n",
        "    gamma=gamma,\n",
        "    tau=tau\n",
        ")\n",
        "\n",
        "record = []\n",
        "frames = []\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        action = agent.select_action(state)\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        agent.store_experience(state, action, reward, next_state, done)\n",
        "        agent.train()\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        if episode == num_episodes - 1:\n",
        "            frames.append(env.render())\n",
        "\n",
        "    record.append(total_reward)\n",
        "    print(f\"Episode {episode + 1}, Total Reward: {total_reward}\")\n",
        "\n",
        "env.close()\n",
        "\n",
        "imageio.mimsave(\"cartpole_last.gif\", frames, fps=30)\n",
        "print(\"GIF saved as cartpole_last.gif\")\n",
        "\n",
        "plt.imshow(frames[0])\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hiH3XWLFlCU-"
      },
      "outputs": [],
      "source": [
        "# plot [episode, reward] history\n",
        "x = [i+1 for i in range(len(record))]\n",
        "plt.plot(x, record)\n",
        "plt.title('episode rewards')\n",
        "plt.xlabel('episodes')\n",
        "plt.ylabel('rewards')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msrPOB7clCU-"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "epsilon_start = 1.0  # Initial epsilon for epsilon-greedy exploration\n",
        "epsilon_final = 0.01  # Final epsilon\n",
        "epsilon_decay = 0.995  # Epsilon decay rate\n",
        "target_update = 10  # Update the target network every 'target_update' steps\n",
        "memory_capacity = 10000  # Replay memory capacity\n",
        "num_episodes = 500  # Number of episodes to train\n",
        "\n",
        "#################################################################\n",
        "#                             TODO                              #\n",
        "#################################################################\n",
        "lr = 1e-3  # Learning rate\n",
        "gamma = 0.99  # Discount factor\n",
        "tau = 0.1 # Parameters update factor\n",
        "batch_size = 64  # Mini-batch size\n",
        "hidden_dims = [20, 20] # Hidden layers\n",
        "#################################################################\n",
        "\n",
        "\n",
        "# Initialize the environment and the agent\n",
        "env = gym.make(\"MountainCar-v0\", render_mode=\"rgb_array\")\n",
        "input_dim = env.observation_space.shape[0]\n",
        "output_dim = env.action_space.n\n",
        "\n",
        "q_network = QNetwork(input_dim, output_dim, hidden_dims)\n",
        "optimizer = optim.Adam(q_network.parameters(), lr=lr)\n",
        "agent = DQNAgent(\n",
        "    q_network=q_network,\n",
        "    optimizer=optimizer,\n",
        "    input_dim=input_dim,\n",
        "    output_dim=output_dim,\n",
        "    epsilon_start=epsilon_start,\n",
        "    epsilon_final=epsilon_final,\n",
        "    epsilon_decay=epsilon_decay,\n",
        "    target_update=target_update,\n",
        "    batch_size=batch_size,\n",
        "    memory_capacity=memory_capacity,\n",
        "    gamma=gamma,\n",
        "    tau=tau\n",
        ")\n",
        "\n",
        "record = []\n",
        "frames = []\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        action = agent.select_action(state)\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        agent.store_experience(state, action, reward, next_state, done)\n",
        "        agent.train()\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        if episode == num_episodes - 1:\n",
        "            frames.append(env.render())\n",
        "\n",
        "    record.append(total_reward)\n",
        "    print(f\"Episode {episode + 1}, Total Reward: {total_reward}\")\n",
        "\n",
        "env.close()\n",
        "\n",
        "imageio.mimsave(\"MountainCar_last.gif\", frames, fps=30)\n",
        "print(\"GIF saved as MountainCar_last.gif\")\n",
        "\n",
        "plt.imshow(frames[0])\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26PDgL0mlCU_"
      },
      "outputs": [],
      "source": [
        "# plot [episode, reward] history\n",
        "x = [i+1 for i in range(len(record))]\n",
        "plt.plot(x, record)\n",
        "plt.title('episode rewards')\n",
        "plt.xlabel('episodes')\n",
        "plt.ylabel('rewards')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "BAE Jae Yong_2019-15668_2281583_assignsubmission_file_Lab 3_BaeJaeyong.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "hyundai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.23"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}